---
title: "CTR_optimization"
author: "Marco Giordano"
date: "12/18/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction [Goals and Why]

This project is just an example of how it is possible to predict the CTR of a website by using Google Search Console data, extracted via Google API. What we present can be considered as a "toy example", in the sense that it is just a demonstration to show two methods on a relatively small dataset. The main reason to predict CTR via Machine Learning techniques is because SEO Specialists usually decide what to optimize by looking at pages with high Impressions and low CTR. However, this method is obviously biased, since CTR varies with position!

I try to test some popular methods to predict the CTR, given a small sample of 25K rows from 3 months from Google Search Console, in order to create a better way to make decisions on what could be optimized. The inspiration came from an article you can find below.

Original Source: [here](https://understandingdata.com/ctr-optimisation-with-machine-learning/)

```{r libraries}
library(randomForest)
library(tidyverse)
library(ggplot2)
library(corrplot)
library(gt)
library(caret)
library(ModelMetrics)
```


```{r, include=FALSE}
setwd('~/Desktop')
gsc_queries_all <- readRDS('sample_gsc')
```

## Descriptive Statistics

As we can quickly notice, clicks and impressions have a high range, due to the effect of most popular articles, featuring very right-skewed distributions too. Data from CTR have the correct values, because the maximum value is equal to 1 and the minimum is not below 0. 
Another large gap may be found by taking a look at the position, which is normal, since we expect that there are some outliers that rank pretty bad for some short-head query, probably.

```{r}
summary(gsc_queries_all[,-c(1:3)])
```


# Data Cleaning

Even though data extracted from GSC API are in a tidy format, there are some small fixes to do before we start with our analysis. For instance, details about the date, the query and the page are not requested to predict the CTR. Hence, we remove them and create a new data set without them.

```{r Data Cleaning, echo=FALSE}
gsc <- gsc_queries_all[,-c(1:3)]

gsc <- gsc %>%
   select(-ctr) %>%
   cbind(gsc_queries_all$ctr) 

colnames(gsc)[4] <- "ctr"
```


## Plotting

Although the data set is not particularly rich in terms of covariates, we can still visualize something to understand its composition. 

```{r 2D Bin}
ggplot(gsc_queries_all, aes(clicks, ctr)) +
  geom_bin2d(bins = 20, color = "white") +
  scale_fill_gradient(low =  "#00AFBB", high = "#FC4E07")+
  theme_minimal()
```

We can use a 2D bin plot to check the relationship between clicks and CTR. It seems that a lot of articles with high CTR tend to have low clicks too. A possible explanation is that pages with higher CTR may rank for long-tail queries that have a low volume of impressions.


To investigate this phenomenon, we can visualize the click distribution, we can use an histogram. We decided to use a vertical line to show the median value, which was 2.00 (check summary). We zoomed on the range 0-100 to have a more clear representation. 

```{r Clicks distribution}
ggplot(gsc_queries_all, aes(clicks, fill = "red", color = "black")) +
  geom_histogram() +
  geom_vline(xintercept = median(gsc_queries_all$clicks), color = "black") +
  xlim(0, 100) +
  theme_minimal() +
  theme(legend.position = "none")
```

It is pretty clear that the distribution is very skewed to the right and 50% of our pages for some queries receive more than 2 clicks. This also translate to 50% of the pages receiving less than 2 clicks, which is coherent with what we found before.

We are interested in getting position as a categorical variable just to plot it. Recall that the position on SERP cannot be continuous, but GSC gives you the average position! We used the ceiling function to round values to the next integer. We didn't use floor to be more 
conservative and avoiding overestimating the position. 

As said in the introduction, CTR varies with position. In our 3-month sample, we can notice that getting the 1st position was crucial in terms of mean CTR and we can easily state that this is similar to what is reported by SEO studies about the relationship between position 
and CTR. Please, note that pages at position 8 got even more than those at 6/7 and as much as 5 in our example. This also tells that aiming for the top spots is truly beneficial, since there is a clear gap in the first 3 positions and even more among position 1 and the rest.

```{r Mean CTR by Average Position}
colors = c("red", "blue", "orange", "yellow", "purple", "lightblue", "pink", "green")
  
gsc_queries_all %>% 
  filter(position <= 8) %>%
  mutate(pos_cat = as.factor(ceiling(position))) %>%
  group_by(pos_cat) %>%
  summarize(mean_ctr = mean(ctr)) %>%
  ungroup() %>%
  ggplot(aes(reorder(pos_cat,desc(pos_cat)), mean_ctr, fill = colors)) +
  geom_col() +
  theme_minimal() +
  coord_flip() +
  theme(legend.position = "none") +
  ggtitle("Mean CTR by Average Position") +
  ylab("Mean CTR") +
  xlab("Average Position")
  
```


## Measuring correlation among variables

We check the correlation matrix to see if there are meaningful associations between variables. We expected a strong correlation between clicks or impressions and CTR, but the results tell us the opposite.

```{r Correlation Plot}
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))

p.mat <- cor.mtest(gsc)$p

corrplot(cor(gsc), , method = "color", col = col(200),
         type = "upper", order = "hclust", number.cex = .7,
         addCoef.col = "black", 
         tl.col = "black", tl.srt = 90,
         p.mat = p.mat, sig.level = 0.01, insig = "blank",
         diag = FALSE)
```
CTR is negatively correlated to clicks but the value is close to 0, so it's not that relevant. On contrary, there is a weak negative correlation between impressions and CTR, which makes sense, because CTR is inversely proportional to impressions. 
The negative correlation between CTR and position is also sound because a lower position is usually associated with a higher CTR. Be careful however, correlation doesn't imply causation, we are just talking about linear association. 


## Train/Test split

Before we start training any model, we need to create a train/test split to avoid overfitting. We will use a 75/25 ratio, chosen arbitrarily and we should have enough data to train a model. To do this, we simply create a random variable to group and partition data.

```{r Train/Test split}
set.seed(55744747)
gp <- runif(nrow(gsc))

train_gsc <- gsc[gp < 0.75,]
test_gsc <- gsc[gp >= 0.75,]
```


After the train/test split, we can scale our two sets to adjust their scale and make algorithms life easier. We use a function from the caret package to process data from the training data set and then apply the same preprocessing to the test data set. Be careful, as you need to scale test data by what seen in train data!

```{r Scale data}
preProcValues <- preProcess(train_gsc[-4], method = c("center", "scale"))

train_gsc[-4] <- predict(preProcValues, train_gsc[-4])
test_gsc[-4] <- predict(preProcValues, test_gsc[-4])
```


## Linear Regression

We start with the most basic model to assess its accuracy. I suppose that this model won't lead to good results because I suspect that the relationship is not linear at all, although linear regression allows for interpolation.
We can directly fit a model with all the covariates.

```{r Perform Linear Regression}
lm_mod <- lm(ctr ~ ., data = train_gsc)
train_Preds <- predict(lm_mod, train_gsc)
Preds <- predict(lm_mod, test_gsc)
```

```{r Summary of Linear Regression}
summary(lm_mod)
```

By taking a quick look at the summary, we can clearly notice that converting variables to log or doing feature engineering would be a waste of time. 

We can visualize our results to assess how well the regression line fits the data points.

```{r Linear Regression}
ggplot(test_gsc, aes(Preds, ctr)) +
  geom_point(alpha = 0.2, color = "darkgray") +
  geom_smooth(color = "darkblue") +
  geom_line(aes(ctr, ctr), color = "blue", linetype = 2) 
```
  
The result is awful but that's perfectly fine, our hypothesis that linear models are not suitable is confirmed. We need something that can capture non-linear relationships and that is more complex. 

We can measure the results of the linear regression with some useful metrics. 

R squared tells us the amount of variance explained by the model and ranges between 0 and 1, where 1 is the best theoretical value. We expect to see R squared values that are close between train and test and that the test one is at least larger than 0.70.

On contrary, RMSE has to be as low as possible since we are talking about errors. Basically, it's the square root of the variance of residuals and refers to the absolute fit of the model to the data. It tells us the accuracy with which our model predicts the response and it is very important for our goal, namely prediction.  
  

Both the two metrics for each of the 2 sets display totally unacceptable values, in line with what we saw before. 


```{r Regression metrics}
reg_rmse_train <- rmse(train_gsc$ctr, train_Preds)
reg_rmse_test <- rmse(test_gsc$ctr, Preds)
reg_R2_train <- 1 - (sum((train_gsc$ctr-train_Preds)^2)/sum((train_gsc$ctr-mean(train_gsc$ctr))^2))
reg_R2_test <- 1 - (sum((test_gsc$ctr-Preds)^2)/sum((test_gsc$ctr-mean(test_gsc$ctr))^2))
```


## Random Forest

The previous result is very disappointing, so we go for an ensemble method that can easily model non-linear relationships and is way more powerful than standard decision trees. I ensure that the experiment is repeatable by setting a seed value.


```{r Fit Random Forest}
set.seed(55744747)
rand_gsc <- randomForest(ctr ~., data = train_gsc, importance = TRUE, ntree = 500)

train_gsc$pred <- predict(rand_gsc, train_gsc)
test_gsc$pred <- predict(rand_gsc, test_gsc)
```

After we fit the model, it is important to create new columns for the predicted values in both the two sets. However, remind that we will only use values from the test data set later on!

```{r Summary of Random Forest}
summary(rand_gsc)
```


```{r Error vs Number of Trees - RF}
plot(rand_gsc, main = "Random Forest")
```
As you can see above, After ~10 trees, error starts to increase and then flatten as we move towards 500. We could also optimize the number of trees used by setting a lower number but the model is already fine this way. Please consider to test different combinations if you need to get better results. In our case, we could just reduce the number of tress to increase the speed of our algorithm.


You may want to take a look at the importance of the variables according to Random Forest. We used the percentage of increase in MSE when a given variable variable is randomly permuted as a measure. 
We can quickly notice that impressions and clicks are essential for our model, although either didn't show any strong correlation with CTR at the beginning of our analysis.
In cases where you have a lot of variables, you can consider using this screening procedure to pick the most important variables and work with a smaller feature set.


```{r Variable importance - RF}
varImp <- importance(rand_gsc)
varImpPlot(rand_gsc, type = 1, main = "Variable Importance")
```


```{r}
R2_train <- 1 - (sum((train_gsc$ctr-train_gsc$pred)^2)/sum((train_gsc$ctr-mean(train_gsc$ctr))^2))
R2_test <- 1 - (sum((test_gsc$ctr-test_gsc$pred)^2)/sum((test_gsc$ctr-mean(test_gsc$ctr))^2))

RMSE_train <- sqrt(mean( (train_gsc$ctr - train_gsc$pred)^2 ))
RMSE_test<- sqrt(mean( (test_gsc$ctr - test_gsc$pred)^2 ))

```


## Hyperparameter tuning for Random Forest

We cold try to improve the performance of our RM model by tuning the mtry (i.e. number of variables randomly sampled as candidate at each split) hyperparameter, if we had more features to work with. We should not see that much difference in this example, since we just have 3 covariates and the default value for regression is set to p/3, where p is the number of features.

```{r Hyperparameter tuning - RF}
mtry <- tuneRF(train_gsc[, -4],train_gsc$ctr, ntreeTry=500,
               stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
```

As expected, the best value for mtry (based on OoB error) is 1, which is already in use when building a model. Consider that this step is essential for bigger models with a larger dimension space. 


```{r}
visual <- tibble(
  Model = c("regression train", "regression test", "RF train", "Rf test"),
  RMSE = c(reg_rmse_train, reg_rmse_test, RMSE_train, RMSE_test),
  R_squared = c(reg_R2_train, reg_R2_test, R2_train, R2_test)
)

visual %>%
  gt() %>%
  tab_header(
    title = md("Summary table with model metrics")
  )
```



## Merge train and test

Now that we have assessed that RM is a valid model, we can merge back train and test data sets, featuring the new columns with predicted values. Then, we can reorder the indexes of the new merged dataframe and create a new column that is given by the difference between the prediction of our model and the actual CTR times 100. Finally, we are ready to combine this dataframe with the one we started with, to recover pages and queries. 


```{r Merge train and test}
df_gsc <- rbind(train_gsc, test_gsc)


df_gsc <- df_gsc[ order(as.numeric(row.names(df_gsc))), ]
df_gsc$diff_perc <- (df_gsc$pred - df_gsc$ctr)*100


final_df <- merge(gsc_queries_all, df_gsc, by = 0, all = T)
```

## Back to the original dataset

We are now ready for the final step. We filter for the rows with values higher than 0 in the percentage difference and then apply some basic transformations. Recall that since we are working with the very first dataset, we don't have scaled values. 

```{r Merge with original dataset}
final_df <- final_df[, -c(1:2, 9:12)]
final_df <- final_df[final_df$diff_perc > 0.00 ,]
final_df <- final_df %>%
  arrange(desc(diff_perc))

final_df$ctr <- final_df$ctr.x * 100
final_df$pred <- final_df$pred * 100

final_df <- final_df %>%
  group_by(page) %>%
  arrange(desc(diff_perc)) %>%
  ungroup() %>%
  select(-c(clicks.x, impressions.x, ctr.x, position.x)) %>%
  relocate(diff_perc, .after = last_col())
```

We will show how our final data set looks like by taking a peak at only the first row since these data are private. 

```{r Visualize result}
head(final_df, 1)
```

Now, we are ready for some interpretation of the results. Our interest is the column of the percentage difference, necessary to understand what has to be optimized. Clearly, we are not interested in the value per se, but in the sign and the relative value compared to others. 
For this reason, arranging rows in descending order by that column is a good to immediately see the pages with bigger differences, meaning that they have a predicted CTR higher than the actual and you should definitely work on them. On contrary, negative values express that those pages are overperforming, so you can leave them be (for the moment).

As mentioned by the original author of this approach, there are some limitations such as the fact that every query is treated as if it was the same. Nonetheless, it is still a better and less biased solution than traditional ones! My personal advice is to use the fitted model just for the website you tested for and not others, since you may want to avoid comparing different data and most likely ranges.

Future improvements may include expanding the original data set with other sources, such as Screaming Frog, Google Analytics or other SEO tools (ie. SEMRush or Hrefs).


## What's next and what to consider

Random Forest cannot do extrapolation, since the average of samples cannot fall outside their ranges. For this reason, you should totally avoid applying this kind of model to new data that fall outside the range on which you trained. Linear Regression can extrapolate data but it performed really poorly and it is not a viable option.

Nonetheless, there are some solutions to try next, for instance neural nets, SVM Regression or Regression-Enhanced Random Forests. The possibility to extrapolate data is something that may improve your CTR optimization process.




