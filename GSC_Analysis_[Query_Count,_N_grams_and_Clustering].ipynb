{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GSC Analysis [Query Count, N-grams and Clustering].ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNJKEI15eBq2MAFWPruIny6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcoGiordano96/DS_works/blob/master/GSC_Analysis_%5BQuery_Count%2C_N_grams_and_Clustering%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyzing data with Google Search Console to find opportunities and assess query count\n",
        "\n",
        "Google Search Console (GSC) is a free tool familiar to many SEO Specialists. However, most of them don't use it to its full potential, which can be exploited via the official API.\n",
        "\n",
        "A common SEO task involves checking GSC to spot opportunities for new queries or to assess your Organic Performance. Using Excel and/or spreadsheets is time consuming and can get in your way if you find to use more advanced functions. Python can solve all these issues and give you much more, as I will show you. You can also use this Google Colab file as a basis for reporting, just change something and comment the due parts and then download it from \"File > Download\".\n",
        "\n",
        "If you want to copy this notebook, go to \"File > Save a copy in Drive\" and you are all set to start.\n",
        "\n",
        "We will mainly cover basic data wrangling and analysis, plus some NLP concepts such as n-grams and association rule mining, a set of convenient techniques to investigate data. But don't worry, you don't need to understand it all now, just be sure to follow what's written and the business purpose/impact of what we have found.\n",
        "\n",
        "This notebook is currently at its first version, but many more updates are planned in the next future. Please, follow the instructions and enjoy the newfound insights!\n",
        "\n",
        "N.B. This notebook took inspiration in some parts from [Hamlet Baptista's notebook](https://colab.research.google.com/github/hamletbatista/sej/blob/master/Hands_on_Introduction_to_Machine_Learning_for_SEOs.ipynb) and [this one from RankSense](https://github.com/ranksense/Twittorials/blob/master/Analyzing_GSC_Data.ipynb).\n"
      ],
      "metadata": {
        "id": "iyVZiFF_NSMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we are able to import data from Google Search Console, there is some necessary setup to be made:\n",
        "\n",
        "1. Having acces to Google Cloud Platform and enabling the **Google Search Console API** from [this link](https://console.cloud.google.com/apis/api/webmasters.googleapis.com/overview?project=&folder=&organizationId=). \n",
        "2. Create new credentials of type OAuth 2.0 in order to request authentication by following [this link](https://console.cloud.google.com/apis/credentials/wizard?api=iamcredentials.googleapis.com&project=).\n",
        "3. Download a copy of the id in .json format, that will be uploaded here in Google Colab."
      ],
      "metadata": {
        "id": "c6PmO0AZIiOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "b6jf197Axyfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3k799pboxJMs"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#load what is needed\n",
        "!pip install git+https://github.com/joshcarty/google-searchconsole\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "from google.colab import data_table\n",
        "!git clone https://github.com/jroakes/querycat.git\n",
        "!pip install -r querycat/requirements_colab.txt\n",
        "!pip install umap-learn\n",
        "data_table.enable_dataframe_formatter() #for better table visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import searchconsole\n",
        "account = searchconsole.authenticate(client_config='client_secret_.json',serialize='credentials.json', flow='console')"
      ],
      "metadata": {
        "id": "4e8v2JGlx8hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "property_name = input('Insert the name of your website as listed in GSC: ')\n",
        "webproperty=account[str(property_name)]"
      ],
      "metadata": {
        "id": "AZqppGbKyOOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can create a function to extract data from one of our properties in Google Search Console. You can select as many dimensions as you want within the fuction, my suggestion is to proceed with query, page and date as we will need them all.\n",
        "\n",
        "Queries alone may not capture all the information we want and that is exactly why we want pages in our dataset too. Dates are useful to understand what happens in the selected timespan."
      ],
      "metadata": {
        "id": "FfZFJlycRi4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_gsc_data(webproperty, start, stop, *args):\n",
        "  if webproperty is not None:\n",
        "    print(f'Extracting data for {webproperty}')\n",
        "    gsc_data = webproperty.query.range(start, stop).dimension(*args).get()\n",
        "    return gsc_data\n",
        "  else:\n",
        "    print('Webproperty not found, please select the correct one')\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "hKOcklq_2V9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example of querying GSC API to get data\n",
        "ex = extract_gsc_data(webproperty, '2021-12-01', '2021-12-01', 'query', 'page', 'date')"
      ],
      "metadata": {
        "id": "LSIOdpPC1r3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data=ex)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "cSMrQ-0x12ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#how many unique pages in the dataset?\n",
        "pages = list(set(df.page))\n",
        "len(pages)"
      ],
      "metadata": {
        "id": "2WV3sXWl75dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we analyze the average CTR per position group; to do so we need to first round the position column."
      ],
      "metadata": {
        "id": "cFAJS7x7kkKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#round position to have accurate groups and then create a pivot table for position and ctr, using the mean to aggregate\n",
        "df['position'] = df['position'].round(0).astype('int64')\n",
        "query_analysis = df.pivot_table(index=['position'], values=['ctr'], aggfunc=['mean'])"
      ],
      "metadata": {
        "id": "DTbU2q3p-oR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.float_format = '{:.2%}'.format\n",
        "query_analysis.sort_values(by=['position'], ascending=True).head(10)"
      ],
      "metadata": {
        "id": "01GIEs61_cKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = query_analysis.head(10).plot(kind='bar')\n",
        "ax.set_xlabel('Position')\n",
        "ax.set_ylabel('CTR')\n",
        "ax.set_title('CTR by position')\n",
        "ax.grid('on')\n",
        "ax.get_legend().remove()"
      ],
      "metadata": {
        "id": "EM7fg0c9_eS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can expect Position 1 to have a much higher share and that is quite normal. This plot is handy to get the general distribution and especially to assess any incongruences. For instance, if position 7 displays a much higher mean value than position 5 you may want to investigate."
      ],
      "metadata": {
        "id": "Rdh1PSwIfSvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query count and SEO efforts\n",
        "\n",
        "As the name implies, query count refers to the process of counting queries and then grouping them by the rounded position. The main idea here is that you can check in which positions you have the most queries and where you have more room for improvement.\n",
        "\n",
        "For instance, if you notice that you have a lot of queries in position 7, you may want to investigate further to see if there are some quick wins of if there isn't much to do."
      ],
      "metadata": {
        "id": "YjRM-RHsOYD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ranking_queries = df.pivot_table(index=['position'], values=['query'], aggfunc=['count'])\n",
        "ranking_queries.sort_values(by=['position']).head(10)"
      ],
      "metadata": {
        "id": "wpFQZuwbC-DC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = ranking_queries.head(10).plot(kind='bar')\n",
        "ax.set_ylabel('Count of queries')\n",
        "ax.set_title('Ranking distribution')\n",
        "ax.grid('on')\n",
        "ax.get_legend().remove()"
      ],
      "metadata": {
        "id": "J8vnB81WDxka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query count is one of the most important actions you can perform with GSC data. Although one can argue that quality trumps over quantity, an increase in query count can be related to SEO success and it is an indicator that Google is valuing more your website.\n",
        "\n",
        "In an ideal scenario, the plot would be shifted leftward as you want to be very high under normal circumstances."
      ],
      "metadata": {
        "id": "z9sWQY8_hRbF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The importance of non-branded queries\n",
        "\n",
        "When measuring your organic performance you should always filter out branded keywords. Nonetheless, it is still of use to know how many branded queries you have and what people are looking for.\n",
        "\n",
        "This section is aimed at understanding the relationship between branded and non-branded keywords."
      ],
      "metadata": {
        "id": "SO3cUfi2P3ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "domain_name = str(input('Insert brand terms separated by a comma: ')).replace(',', '|')\n",
        "import re\n",
        "domain_name = re.sub(r\"\\s+\", \"\", domain_name)\n",
        "print('Remove all spaces using RegEx:\\n')\n",
        "df['Brand/Non-branded'] = np.where(\n",
        "    df['query'].str.contains(domain_name), 'Brand', 'Non-branded'\n",
        ")"
      ],
      "metadata": {
        "id": "q_26n3mkEEBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brand_count_df = df['Brand/Non-branded'].value_counts().rename_axis('cats').to_frame('counts')"
      ],
      "metadata": {
        "id": "EMPpf0ZREQVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brand_count_df['Percentage'] = brand_count_df['counts']/sum(brand_count_df['counts'])"
      ],
      "metadata": {
        "id": "3y_0YB_Ksx1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.float_format = '{:.2%}'.format\n",
        "brand_count_df"
      ],
      "metadata": {
        "id": "KdWL14Kdwwqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Brand/Non-branded'].value_counts().plot(kind='bar')"
      ],
      "metadata": {
        "id": "GUYxGJm3kWOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we move on, it is very suggested to filter out branded keywords as they won't be part of our next analyses."
      ],
      "metadata": {
        "id": "W9QjNZMrq4Pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#only select non-branded keywords\n",
        "df = df.loc[df['Brand/Non-branded'] == 'Non-branded']"
      ],
      "metadata": {
        "id": "o-g_ovWnqsIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLP: how to leverage basic functions\n"
      ],
      "metadata": {
        "id": "-GOeIEAmTfZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Natural Language Processing (NLP) is one of the most interesting subsets of Machine Learning that are of interest for any SEO practitioner. Python is quite convenient for NLP tasks as there are plenty of packages and modules available.\n",
        "\n",
        "One of the first tasks that comes nifty with this type of text data is n-gram analysis. In short, we want to know which are the most common sequence of words across our queries."
      ],
      "metadata": {
        "id": "Vld4qf1BiGO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "textlist = df['query'].to_list()\n",
        "pd.Series(textlist).value_counts().head(10)"
      ],
      "metadata": {
        "id": "2rQUfrg5Ucu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "x = Counter(textlist)\n",
        "x.most_common(10)"
      ],
      "metadata": {
        "id": "14wrhDDVWHtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download stopwords list to remove what is not needed\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stoplist = stopwords.words('english')"
      ],
      "metadata": {
        "id": "RSmt31tUWOZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This very useful snippet of code was borrowed from this [Medium article](https://towardsdatascience.com/text-analysis-basics-in-python-443282942ec5://), which a good introduction to text analysis in Python."
      ],
      "metadata": {
        "id": "UxO5CyiOw8-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create dataframe with bigrams and trigrams\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "c_vec = CountVectorizer(stop_words=stoplist, ngram_range=(2,3)) #can also select bigrams only\n",
        "# matrix of ngrams\n",
        "ngrams = c_vec.fit_transform(df['query'])\n",
        "# count frequency of ngrams\n",
        "count_values = ngrams.toarray().sum(axis=0)\n",
        "# list of ngrams\n",
        "vocab = c_vec.vocabulary_\n",
        "df_ngram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)\n",
        "            ).rename(columns={0: 'frequency', 1:'bigram/trigram'})"
      ],
      "metadata": {
        "id": "aIRfidHnWvcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyzing bigrams and trigrams is useful to understand your most popular topics, in a way. Using other techniques to cluster groups of words is definitely more effective, although n-gram analysis is still very practical and useful to identify clusters."
      ],
      "metadata": {
        "id": "T4fMRPcadSZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ngram.head(20).style.background_gradient()"
      ],
      "metadata": {
        "id": "xVqnfnJxYTea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to be more granular, you can check those queries that are above the 80th quantile for impressions and below the 20th quantile for CTR. In other words, we are looking for those queries who have more impressions than the other 80% of the dataset but have way worse CTR compared to 80% of the dataset."
      ],
      "metadata": {
        "id": "IiuidOgEtFGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_impressions = df[df['impressions'] >= df['impressions'].quantile(0.8)]\n",
        "(top_impressions[top_impressions['ctr'] <= top_impressions['ctr'].quantile(0.2)].sort_values('impressions', ascending = False))"
      ],
      "metadata": {
        "id": "JxcAhwkwOy4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querycat to the rescue\n",
        "A more effective way to tackle clustering is offered by the querycat library, capable of classifying queries with the help of association rule learning. We have already loaded and installed all the needed packages before so we are now ready to go!\n",
        "\n",
        "NB: I will update this section to include the FP Growth algorithm, which is generally faster than the one I am going to show you."
      ],
      "metadata": {
        "id": "8JdMQF9n0wH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import querycat"
      ],
      "metadata": {
        "id": "6nWXGLiKKLKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explaining querycat is not as straightforward as it seems as you may need to know more about some algorithms like the apriori one. For the moment, you just have to know we are using the apriori algorith with a value of 10 for the minimum support, which is quite good in terms of results.\n",
        "\n",
        "The apriori algorithm is needed for finding similarities across rows and create categories based on that. Therefore, this methodology is also very useful when handling products  and/or categories in an e-commerce scenario, although we will use queries in this specific case."
      ],
      "metadata": {
        "id": "Mewb6KWLO9c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_cat = querycat.Categorize(df, 'query', min_support=10, alg='apriori')"
      ],
      "metadata": {
        "id": "evhJri6G2ABI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may happen to have a lot of clusters and some of them may be small. You can set a filter to remove them."
      ],
      "metadata": {
        "id": "dQpGxrBjPcuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#if you have too many clusters, you can try to set a condition to remove those with very low clicks and suchlike\n",
        "dfgrouped = df.groupby('category').agg(sumclicks = ('clicks', 'sum')).sort_values('sumclicks', ascending=False)\n",
        "filtergroup = dfgrouped[dfgrouped['sumclicks'] > 15] #arbitrary threshold\n",
        "filtergroup"
      ],
      "metadata": {
        "id": "Zu9qIsiH2ALl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merge original dataframe with filtered one\n",
        "df = df.merge(filtergroup, on=['category','category'], how='inner')"
      ],
      "metadata": {
        "id": "X2naw2iK4h3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['category'].value_counts()"
      ],
      "metadata": {
        "id": "Fmb_7rl65Jjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('category').sum()['clicks'].sort_values(ascending=False).head(10)"
      ],
      "metadata": {
        "id": "C7Uq_fns7YsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are interested in getting queries for each cluster along with total clicks and total impressions. To do this, we can create two variables storing the sum of clicks and impressions per group and then merge them to the other dataset."
      ],
      "metadata": {
        "id": "beaQJ8T6xTdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('df_gsc.csv')\n",
        "files.download('df_gsc.csv')"
      ],
      "metadata": {
        "id": "05Hj9Bjn4Fpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_df = df.groupby('category')[['clicks', 'impressions']].agg('sum')"
      ],
      "metadata": {
        "id": "n6-mt39X3tT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group_ex = df.groupby(['category'])['query'].apply(' | '.join).reset_index()\n",
        "#remove duplicate queries and then sort them alphabetically\n",
        "group_ex['query'] = group_ex['query'].apply(lambda x: ' | '.join(sorted(list(set(x.split('|'))))))"
      ],
      "metadata": {
        "id": "BzzewFIEFh36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#final dataset ready to be exported\n",
        "df_final = group_ex.merge(grouped_df, on=['category', 'category'], how='inner')\n",
        "df_final.head()"
      ],
      "metadata": {
        "id": "YfTVAHuB30VR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save csv file and download it to your local machine. If you use Safari, please consider switching to Chrome for downloading these files as it may not work.\n",
        "df_final.to_csv('clusters_queries.csv')\n",
        "files.download('clusters_queries.csv')"
      ],
      "metadata": {
        "id": "at474gxzUlZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you have a list of queries grouped by query with clicks and impressions to guide you in the decision process. Groups with a high impression count may be worth investigating, as you can find new opportunities or see if you can optimize for some queries, starting from your topical map. Please recall that optimizing for single queries alone is not enough anymore, and you should instead focus on organized groups of content where everything is interconnected via significant internal links with proper anchor texts.\n",
        "\n",
        "In fact, this notebook should guide you in finding possible content gaps or to check how your clusters are performing.\n",
        "\n",
        "For more information about this notebook or if you want to collaborate or provide me with a feedback, you can contact me on [Twitter](https://twitter.com/giordmarco96/) or [LinkedIn](https://www.linkedin.com/in/marco-giordano96)."
      ],
      "metadata": {
        "id": "_rXVoTgTyAbv"
      }
    }
  ]
}